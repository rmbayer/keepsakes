{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting with Scikit-Learn, XGBoost, LightGBM, and CatBoost\n",
    "\n",
    "Written by Jason Brownlee, PhD - Notebook by Robert Bayer\n",
    "\n",
    "It’s popular for structured predictive modeling problems, such as classification and regression on tabular data, and is often the main algorithm or one of the main algorithms used in winning solutions to machine learning competitions, like those on Kaggle.\n",
    "\n",
    "There are many implementations of gradient boosting available, including standard implementations in SciPy and efficient third-party libraries. Each uses a different interface and even different names for the algorithm.\n",
    "\n",
    "In this tutorial, you will discover how to use gradient boosting models for classification and regression in Python.\n",
    "\n",
    "Standardized code examples are provided for the four major implementations of gradient boosting in Python, ready for you to copy-paste and use in your own predictive modeling project.\n",
    "\n",
    "After completing this tutorial, you will know:\n",
    "\n",
    "* Gradient boosting is an ensemble algorithm that fits boosted decision trees by minimizing an error gradient.\n",
    "* How to evaluate and use gradient boosting with scikit-learn, including gradient boosting machines and the histogram-based algorithm.\n",
    "* How to evaluate and use third-party gradient boosting algorithms, including XGBoost, LightGBM, and CatBoost.\n",
    "Let’s get started.\n",
    "\n",
    "# Tutorial Overview\n",
    "\n",
    "This tutorial is divided into five parts; they are:\n",
    "\n",
    "1. Gradient Boosting Overview\n",
    "2. Gradient Boosting With Scikit-Learn\n",
    "    1. Library Installation\n",
    "    2. Test Problems\n",
    "    3. Gradient Boosting\n",
    "    4. Histogram-Based Gradient Boosting\n",
    "3. Gradient Boosting With XGBoost\n",
    "    1. Library Installation\n",
    "    2. XGBoost for Classification\n",
    "    3. XGBoost for Regression\n",
    "4. Gradient Boosting With LightGBM\n",
    "    1. Library Installation\n",
    "    2. LightGBM for Classification\n",
    "    3. LightGBM for Regression\n",
    "5. Gradient Boosting With CatBoost\n",
    "    1. Library Installation\n",
    "    2. CatBoost for Classification\n",
    "    3. CatBoost for Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Overview\n",
    "\n",
    "Gradient boosting refers to a class of ensemble machine learning algorithms that can be used for classification or regression predictive modeling problems.\n",
    "\n",
    "Gradient boosting is also known as gradient tree boosting, stochastic gradient boosting (an extension), and gradient boosting machines, or GBM for short.\n",
    "\n",
    "Ensembles are constructed from decision tree models. Trees are added one at a time to the ensemble and fit to correct the prediction errors made by prior models. This is a type of ensemble machine learning model referred to as boosting.\n",
    "\n",
    "Models are fit using any arbitrary differentiable loss function and gradient descent optimization algorithm. This gives the technique its name, “gradient boosting,” as the loss gradient is minimized as the model is fit, much like a neural network.\n",
    "\n",
    "Gradient boosting is an effective machine learning algorithm and is often the main, or one of the main, algorithms used to win machine learning competitions (like Kaggle) on tabular and similar structured datasets.\n",
    "\n",
    "**Note:** We will not be going into the theory behind how the gradient boosting algorithm works in this tutorial.\n",
    "\n",
    "For more on the gradient boosting algorithm, see the tutorial:\n",
    "\n",
    "* [A Gentle Introduction to the Gradient Boosting Algorithm for Machine Learning](https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/)\n",
    "\n",
    "The algorithm provides hyperparameters that should, and perhaps must, be tuned for a specific dataset. Although there are many hyperparameters to tune, perhaps the most important are as follows:\n",
    "\n",
    "* The number of trees or estimators in the model.\n",
    "* The learning rate of the model.\n",
    "* The row and column sampling rate for stochastic models.\n",
    "* The maximum tree depth.\n",
    "* The minimum tree weight.\n",
    "* The regularization terms alpha and lambda.\n",
    "\n",
    "**Note:** We will not be exploring how to configure or tune the configuration of gradient boosting algorithms in this tutorial.\n",
    "\n",
    "For more on tuning the hyperparameters of gradient boosting algorithms, see the tutorial:\n",
    "\n",
    "* [How to Configure the Gradient Boosting Algorithm](https://machinelearningmastery.com/configure-gradient-boosting-algorithm/)\n",
    "\n",
    "There are many implementations of the gradient boosting algorithm available in Python. Perhaps the most used implementation is the version provided with the scikit-learn library.\n",
    "\n",
    "Additional third-party libraries are available that provide computationally efficient alternate implementations of the algorithm that often achieve better results in practice. Examples include the XGBoost library, the LightGBM library, and the CatBoost library.\n",
    "\n",
    "When using gradient boosting on your predictive modeling project, you may want to test each implementation of the algorithm.\n",
    "\n",
    "This tutorial provides examples of each implementation of the gradient boosting algorithm on classification and regression predictive modeling problems that you can copy-paste into your project.\n",
    "\n",
    "Let’s take a look at each in turn.\n",
    "\n",
    "**Note:** We are not comparing the performance of the algorithms in this tutorial. Instead, we are providing code examples to demonstrate how to use each different implementation. As such, we are using synthetic test datasets to demonstrate evaluating and making a prediction with each implementation.\n",
    "\n",
    "This tutorial assumes you have Python and SciPy installed. If you need help, see the tutorial:\n",
    "\n",
    "* [How to Setup Your Python Environment for Machine Learning with Anaconda](https://machinelearningmastery.com/setup-python-environment-machine-learning-deep-learning-anaconda/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting with Scikit-Learn\n",
    "\n",
    "In this section, we will review how to use the gradient boosting algorithm implementation in the [scikit-learn library](https://scikit-learn.org/).\n",
    "\n",
    "## Library Installation\n",
    "\n",
    "First, let’s install the library.\n",
    "\n",
    "Don’t skip this step as you will need to ensure you have the latest version installed.\n",
    "\n",
    "You can install the scikit-learn library using the pip Python installer, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For additional installation instructions specific to your platform, see:\n",
    "\n",
    "* [Installing scikit-learn](https://scikit-learn.org/stable/install.html)\n",
    "\n",
    "Next, let’s confirm that the library is installed and you are using a modern version.\n",
    "\n",
    "Run the following script to print the library version number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check scikit-learn version\n",
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example, you should see the following version number or higher.\n",
    "\n",
    "> 0.22.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Problems\n",
    "\n",
    "We will demonstrate the gradient boosting algorithm for classification and regression.\n",
    "\n",
    "As such, we will use synthetic test problems from the scikit-learn library.\n",
    "\n",
    "## Classification Dataset\n",
    "\n",
    "We will use the [make_classification() function](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) to create a test binary classification dataset.\n",
    "\n",
    "The dataset will have 1,000 examples, with 10 input features, five of which will be informative and the remaining five that will be redundant. We will fix the random number seed to ensure we get the same examples each time the code is run.\n",
    "\n",
    "An example of creating and summarizing the dataset is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test classification dataset\n",
    "from sklearn.datasets import make_classification\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n",
    "# summarize the dataset\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Dataset\n",
    "We will use the [make_regression() function](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html) to create a test regression dataset.\n",
    "\n",
    "Like the classification dataset, the regression dataset will have 1,000 examples, with 10 input features, five of which will be informative and the remaining five that will be redundant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test regression dataset\n",
    "from sklearn.datasets import make_regression\n",
    "# define dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=10, n_informative=5, random_state=1)\n",
    "# summarize the dataset\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unning the example creates the dataset and confirms the expected number of samples and features\n",
    "\n",
    "> (1000,10) (1000,)\n",
    "\n",
    "Next, let’s look at how we can develop gradient boosting models in scikit-learn.\n",
    "\n",
    "# Gradient Boosting\n",
    "The scikit-learn library provides the GBM algorithm for regression and classification via the _GradientBoostingClassifier_ and _GradientBoostingRegressor_ classes.\n",
    "\n",
    "Let’s take a closer look at each in turn.\n",
    "\n",
    "Gradient Boosting Machine for Classification\n",
    "The example below first evaluates a [GradientBoostingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) on the test problem using repeated k-fold cross-validation and reports the mean accuracy. Then a single model is fit on all available data and a single prediction is made.\n",
    "\n",
    "The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient boosting for classification in scikit-learn\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from matplotlib import pyplot\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n",
    "# evaluate the model\n",
    "model = GradientBoostingClassifier()\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n",
    "# fit the model on the whole dataset\n",
    "model = GradientBoostingClassifier()\n",
    "model.fit(X, y)\n",
    "# make a single prediction\n",
    "row = [[2.56999479, -0.13019997, 3.16075093, -4.35936352, -1.61271951, -1.39352057, -2.48924933, -1.93094078, 3.26130366, 2.05692145]]\n",
    "yhat = model.predict(row)\n",
    "print('Prediction: %d' % yhat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n",
    "\n",
    "Running the example first reports the evaluation of the model using repeated k-fold cross-validation, then the result of making a single prediction with a model fit on the entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Accuracy: 0.915 (0.025)<br> Prediction: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Machine for Regression\n",
    "The example below first evaluates a [GradientBoostingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html) on the test problem using repeated k-fold cross-validation and reports the mean absolute error. Then a single model is fit on all available data and a single prediction is made.\n",
    "\n",
    "The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient boosting for regression in scikit-learn\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from matplotlib import pyplot\n",
    "# define dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=10, n_informative=5, random_state=1)\n",
    "# evaluate the model\n",
    "model = GradientBoostingRegressor()\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\n",
    "print('MAE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n",
    "# fit the model on the whole dataset\n",
    "model = GradientBoostingRegressor()\n",
    "model.fit(X, y)\n",
    "# make a single prediction\n",
    "row = [[2.02220122, 0.31563495, 0.82797464, -0.30620401, 0.16003707, -1.44411381, 0.87616892, -0.50446586, 0.23009474, 0.76201118]]\n",
    "yhat = model.predict(row)\n",
    "print('Prediction: %.3f' % yhat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example first reports the evaluation of the model using repeated k-fold cross-validation, then the result of making a single prediction with a model fit on the entire dataset.\n",
    "\n",
    "> MAE: -11.854 (1.121)<br>Prediction: -80.661"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram-Based Gradient Boosting\n",
    "The scikit-learn library provides an alternate implementation of the gradient boosting algorithm, referred to as histogram-based gradient boosting.\n",
    "\n",
    "This is an alternate approach to implement gradient tree boosting inspired by the LightGBM library (described more later). This implementation is provided via the _HistGradientBoostingClassifier_ and _HistGradientBoostingRegressor_ classes.\n",
    "\n",
    "The primary benefit of the histogram-based approach to gradient boosting is speed. These implementations are designed to be much faster to fit on training data.\n",
    "\n",
    "At the time of writing, this is an experimental implementation and requires that you add the following line to your code to enable access to these classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_hist_gradient_boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without this line, you will see an error like:\n",
    "> ImportError: cannot import name 'HistGradientBoostingClassifier'\n",
    "\n",
    "or\n",
    "\n",
    "> ImportError: cannot import name 'HistGradientBoostingRegressor'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram-Based Gradient Boosting Machine for Classification\n",
    "The example below first evaluates a [HistGradientBoostingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html) on the test problem using repeated k-fold cross-validation and reports the mean accuracy. Then a single model is fit on all available data and a single prediction is made.\n",
    "\n",
    "The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram-based gradient boosting for classification in scikit-learn\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from matplotlib import pyplot\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n",
    "# evaluate the model\n",
    "model = HistGradientBoostingClassifier()\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n",
    "# fit the model on the whole dataset\n",
    "model = HistGradientBoostingClassifier()\n",
    "model.fit(X, y)\n",
    "# make a single prediction\n",
    "row = [[2.56999479, -0.13019997, 3.16075093, -4.35936352, -1.61271951, -1.39352057, -2.48924933, -1.93094078, 3.26130366, 2.05692145]]\n",
    "yhat = model.predict(row)\n",
    "print('Prediction: %d' % yhat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example first reports the evaluation of the model using repeated k-fold cross-validation, then the result of making a single prediction with a model fit on the entire dataset\n",
    "\n",
    "> Accuracy: 0.935 (0.024)<br>Prediction: 1\n",
    "\n",
    "## Histogram-Based Gradient Boosting Machine for Regression\n",
    "The example below first evaluates a [HistGradientBoostingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html) on the test problem using repeated k-fold cross-validation and reports the mean absolute error. Then a single model is fit on all available data and a single prediction is made.\n",
    "\n",
    "The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram-based gradient boosting for regression in scikit-learn\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from matplotlib import pyplot\n",
    "# define dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=10, n_informative=5, random_state=1)\n",
    "# evaluate the model\n",
    "model = HistGradientBoostingRegressor()\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\n",
    "print('MAE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n",
    "# fit the model on the whole dataset\n",
    "model = HistGradientBoostingRegressor()\n",
    "model.fit(X, y)\n",
    "# make a single prediction\n",
    "row = [[2.02220122, 0.31563495, 0.82797464, -0.30620401, 0.16003707, -1.44411381, 0.87616892, -0.50446586, 0.23009474, 0.76201118]]\n",
    "yhat = model.predict(row)\n",
    "print('Prediction: %.3f' % yhat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example first reports the evaluation of the model using repeated k-fold cross-validation, then the result of making a single prediction with a model fit on the entire dataset.\n",
    "\n",
    "> MAE: -12.723 (1.540)<br>Prediction: -77.837\n",
    "\n",
    "# Gradient Boosting With XGBoost\n",
    "[XGBoost](https://xgboost.ai/), which is short for _“Extreme Gradient Boosting,”_ is a library that provides an efficient implementation of the gradient boosting algorithm.\n",
    "\n",
    "The main benefit of the XGBoost implementation is computational efficiency and often better model performance.\n",
    "\n",
    "For more on the benefits and capability of XGBoost, see the tutorial:\n",
    "\n",
    "* [A Gentle Introduction to XGBoost for Applied Machine Learning](https://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/)\n",
    "\n",
    "## Library Installation\n",
    "You can install the XGBoost library using the pip Python installer, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For additional installation instructions specific to your platform see:\n",
    "\n",
    "* [XGBoost Installation Guide](https://xgboost.readthedocs.io/en/latest/build.html)\n",
    "\n",
    "Next, let’s confirm that the library is installed and you are using a modern version.\n",
    "\n",
    "Run the following script to print the library version number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check xgboost version\n",
    "import xgboost\n",
    "print(xgboost.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example, you should see the following version number or higher.\n",
    "> 1.0.1\n",
    "\n",
    "The XGBoost library provides wrapper classes so that the efficient algorithm implementation can be used with the scikit-learn library, specifically via the XGBClassifier and XGBregressor classes.\n",
    "\n",
    "Let’s take a closer look at each in turn.\n",
    "\n",
    "# XGBoost for Classification\n",
    "The example below first evaluates an [XGBClassifier](https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBClassifier) on the test problem using repeated k-fold cross-validation and reports the mean accuracy. Then a single model is fit on all available data and a single prediction is made.\n",
    "\n",
    "The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost for classification\n",
    "from numpy import asarray\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from matplotlib import pyplot\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n",
    "# evaluate the model\n",
    "model = XGBClassifier()\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n",
    "# fit the model on the whole dataset\n",
    "model = XGBClassifier()\n",
    "model.fit(X, y)\n",
    "# make a single prediction\n",
    "row = [2.56999479, -0.13019997, 3.16075093, -4.35936352, -1.61271951, -1.39352057, -2.48924933, -1.93094078, 3.26130366, 2.05692145]\n",
    "row = asarray(row).reshape((1, len(row)))\n",
    "yhat = model.predict(row)\n",
    "print('Prediction: %d' % yhat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example first reports the evaluation of the model using repeated k-fold cross-validation, then the result of making a single prediction with a model fit on the entire dataset.\n",
    "\n",
    "> Accuracy: 0.936 (0.019)<br>Prediction: 1\n",
    "\n",
    "# XGBoost for Regression\n",
    "The example below first evaluates an [XGBRegressor](https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBRegressor) on the test problem using repeated k-fold cross-validation and reports the mean absolute error. Then a single model is fit on all available data and a single prediction is made.\n",
    "\n",
    "The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost for regression\n",
    "from numpy import asarray\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_regression\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from matplotlib import pyplot\n",
    "# define dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=10, n_informative=5, random_state=1)\n",
    "# evaluate the model\n",
    "model = XGBRegressor(objective='reg:squarederror')\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\n",
    "print('MAE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n",
    "# fit the model on the whole dataset\n",
    "model = XGBRegressor(objective='reg:squarederror')\n",
    "model.fit(X, y)\n",
    "# make a single prediction\n",
    "row = [2.02220122, 0.31563495, 0.82797464, -0.30620401, 0.16003707, -1.44411381, 0.87616892, -0.50446586, 0.23009474, 0.76201118]\n",
    "row = asarray(row).reshape((1, len(row)))\n",
    "yhat = model.predict(row)\n",
    "print('Prediction: %.3f' % yhat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example first reports the evaluation of the model using repeated k-fold cross-validation, then the result of making a single prediction with a model fit on the entire dataset.\n",
    "\n",
    "> MAE: -15.048 (1.316)<br>Prediction: -93.434\n",
    "\n",
    "# Gradient Boosting With LightGBM\n",
    "[LightGBM](https://github.com/microsoft/LightGBM), short for Light Gradient Boosted Machine, is a library developed at Microsoft that provides an efficient implementation of the gradient boosting algorithm.\n",
    "\n",
    "The primary benefit of the LightGBM is the changes to the training algorithm that make the process dramatically faster, and in many cases, result in a more effective model.\n",
    "\n",
    "For more technical details on the LightGBM algorithm, see the paper:\n",
    "\n",
    "* [LightGBM: A Highly Efficient Gradient Boosting Decision Tree](https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree), 2017.\n",
    "\n",
    "## Library Installation\n",
    "You can install the LightGBM library using the pip Python installer, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo pip install lightgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For additional installation instructions specific to your platform, see:\n",
    "\n",
    "* [LightGBM Installation Guide](https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html)\n",
    "\n",
    "Next, let’s confirm that the library is installed and you are using a modern version.\n",
    "\n",
    "Run the following script to print the library version number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check lightgbm version\n",
    "import lightgbm\n",
    "print(lightgbm.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example, you should see the following version number or higher.\n",
    "\n",
    "> 2.3.1\n",
    "\n",
    "The LightGBM library provides wrapper classes so that the efficient algorithm implementation can be used with the scikit-learn library, specifically via the _LGBMClassifier_ and _LGBMRegressor_ classes.\n",
    "\n",
    "Let’s take a closer look at each in turn.\n",
    "\n",
    "# LightGBM for Classification\n",
    "The example below first evaluates an [LGBMClassifier](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html) on the test problem using repeated k-fold cross-validation and reports the mean accuracy. Then a single model is fit on all available data and a single prediction is made.\n",
    "\n",
    "The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lightgbm for classification\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from matplotlib import pyplot\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n",
    "# evaluate the model\n",
    "model = LGBMClassifier()\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n",
    "# fit the model on the whole dataset\n",
    "model = LGBMClassifier()\n",
    "model.fit(X, y)\n",
    "# make a single prediction\n",
    "row = [[2.56999479, -0.13019997, 3.16075093, -4.35936352, -1.61271951, -1.39352057, -2.48924933, -1.93094078, 3.26130366, 2.05692145]]\n",
    "yhat = model.predict(row)\n",
    "print('Prediction: %d' % yhat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example first reports the evaluation of the model using repeated k-fold cross-validation, then the result of making a single prediction with a model fit on the entire dataset.\n",
    "\n",
    "> Accuracy: 0.934 (0.021)<br>Prediction: 1\n",
    "\n",
    "# LightGBM for Regression\n",
    "The example below first evaluates an [LGBMRegressor](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html) on the test problem using repeated k-fold cross-validation and reports the mean absolute error. Then a single model is fit on all available data and a single prediction is made.\n",
    "\n",
    "The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lightgbm for regression\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_regression\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from matplotlib import pyplot\n",
    "# define dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=10, n_informative=5, random_state=1)\n",
    "# evaluate the model\n",
    "model = LGBMRegressor()\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\n",
    "print('MAE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n",
    "# fit the model on the whole dataset\n",
    "model = LGBMRegressor()\n",
    "model.fit(X, y)\n",
    "# make a single prediction\n",
    "row = [[2.02220122, 0.31563495, 0.82797464, -0.30620401, 0.16003707, -1.44411381, 0.87616892, -0.50446586, 0.23009474, 0.76201118]]\n",
    "yhat = model.predict(row)\n",
    "print('Prediction: %.3f' % yhat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example first reports the evaluation of the model using repeated k-fold cross-validation, then the result of making a single prediction with a model fit on the entire dataset.\n",
    "\n",
    "> MAE: -12.739 (1.408)<br>Prediction: -82.040\n",
    "\n",
    "# Gradient Boosting with CatBoost\n",
    "[CatBoost](https://catboost.ai/) is a third-party library developed at [Yandex](https://en.wikipedia.org/wiki/Yandex) that provides an efficient implementation of the gradient boosting algorithm.\n",
    "\n",
    "The primary benefit of the CatBoost (in addition to computational speed improvements) is support for categorical input variables. This gives the library its name CatBoost for _“Category Gradient Boosting.”_\n",
    "\n",
    "For more technical details on the CatBoost algorithm, see the paper:\n",
    "\n",
    "* [CatBoost: gradient boosting with categorical features support](https://arxiv.org/abs/1810.11363), 2017.\n",
    "\n",
    "## Library Installation\n",
    "You can install the CatBoost library using the pip Python installer, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo pip install catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For additional installation instructions specific to your platform, see:\n",
    "\n",
    "* [CatBoost Installation Guide](https://catboost.ai/docs/concepts/python-installation.html)\n",
    "\n",
    "Next, let’s confirm that the library is installed and you are using a modern version.\n",
    "\n",
    "Run the following script to print the library version number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check catboost version\n",
    "import catboost\n",
    "print(catboost.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example, you should see the following version number or higher.\n",
    "\n",
    "> 0.21\n",
    "\n",
    "The CatBoost library provides wrapper classes so that the efficient algorithm implementation can be used with the scikit-learn library, specifically via the _CatBoostClassifier_ and _CatBoostRegressor_ classes.\n",
    "\n",
    "Let’s take a closer look at each in turn.\n",
    "\n",
    "# CatBoost for Classification\n",
    "The example below first evaluates a [CatBoostClassifier](https://catboost.ai/docs/concepts/python-reference_catboostclassifier.html) on the test problem using repeated k-fold cross-validation and reports the mean accuracy. Then a single model is fit on all available data and a single prediction is made.\n",
    "\n",
    "The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# catboost for classification\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from matplotlib import pyplot\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n",
    "# evaluate the model\n",
    "model = CatBoostClassifier(verbose=0, n_estimators=100)\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n",
    "# fit the model on the whole dataset\n",
    "model = CatBoostClassifier(verbose=0, n_estimators=100)\n",
    "model.fit(X, y)\n",
    "# make a single prediction\n",
    "row = [[2.56999479, -0.13019997, 3.16075093, -4.35936352, -1.61271951, -1.39352057, -2.48924933, -1.93094078, 3.26130366, 2.05692145]]\n",
    "yhat = model.predict(row)\n",
    "print('Prediction: %d' % yhat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example first reports the evaluation of the model using repeated k-fold cross-validation, then the result of making a single prediction with a model fit on the entire dataset.\n",
    "\n",
    "> Accuracy: 0.931 (0.026)<br>Prediction: 1\n",
    "\n",
    "# CatBoost for Regression\n",
    "The example below first evaluates a [CatBoostRegressor](https://catboost.ai/docs/concepts/python-reference_catboostregressor.html) on the test problem using repeated k-fold cross-validation and reports the mean absolute error. Then a single model is fit on all available data and a single prediction is made.\n",
    "\n",
    "The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# catboost for regression\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_regression\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from matplotlib import pyplot\n",
    "# define dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=10, n_informative=5, random_state=1)\n",
    "# evaluate the model\n",
    "model = CatBoostRegressor(verbose=0, n_estimators=100)\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\n",
    "print('MAE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n",
    "# fit the model on the whole dataset\n",
    "model = CatBoostRegressor(verbose=0, n_estimators=100)\n",
    "model.fit(X, y)\n",
    "# make a single prediction\n",
    "row = [[2.02220122, 0.31563495, 0.82797464, -0.30620401, 0.16003707, -1.44411381, 0.87616892, -0.50446586, 0.23009474, 0.76201118]]\n",
    "yhat = model.predict(row)\n",
    "print('Prediction: %.3f' % yhat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example first reports the evaluation of the model using repeated k-fold cross-validation, then the result of making a single prediction with a model fit on the entire dataset.\n",
    "\n",
    "> MAE: -9.281 (0.951)<br>Prediction: -74.212\n",
    "\n",
    "# Further Reading\n",
    "This section provides more resources on the topic if you are looking to go deeper.\n",
    "\n",
    "## Tutorials\n",
    "* [How to Setup Your Python Environment for Machine Learning with Anaconda](https://machinelearningmastery.com/setup-python-environment-machine-learning-deep-learning-anaconda/)\n",
    "* [A Gentle Introduction to the Gradient Boosting Algorithm for Machine Learning](https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/)\n",
    "* [How to Configure the Gradient Boosting Algorithm]https://machinelearningmastery.com/configure-gradient-boosting-algorithm/()\n",
    "* [A Gentle Introduction to XGBoost for Applied Machine Learning](https://arxiv.org/abs/1810.11363)\n",
    "## Papers\n",
    "* [Stochastic Gradient Boosting, 2002.](https://www.sciencedirect.com/science/article/pii/S0167947301000652)\n",
    "* [XGBoost: A Scalable Tree Boosting System, 2016.](https://arxiv.org/abs/1603.02754)\n",
    "* [LightGBM: A Highly Efficient Gradient Boosting Decision Tree, 2017.](https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree)\n",
    "* [CatBoost: gradient boosting with categorical features support, 2017.](https://arxiv.org/abs/1810.11363)\n",
    "## APIs\n",
    "* [Scikit-Learn Homepage.](https://scikit-learn.org/)\n",
    "* [sklearn.ensemble API.](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble)\n",
    "* [XGBoost Homepage.](https://xgboost.ai/)\n",
    "* [XGBoost Python API.](https://xgboost.readthedocs.io/en/latest/python/python_api.html)\n",
    "* [LightGBM Project.](https://github.com/microsoft/LightGBM)\n",
    "* [LightGBM Python API.](https://lightgbm.readthedocs.io/en/latest/Python-API.html)\n",
    "* [CatBoost Homepage.](https://catboost.ai/)\n",
    "* [CatBoost API.](https://catboost.ai/docs/)\n",
    "## Articles\n",
    "* [Gradient boosting, Wikipedia.](https://en.wikipedia.org/wiki/Gradient_boosting)\n",
    "* [XGBoost, Wikipedia.](https://en.wikipedia.org/wiki/XGBoost)\n",
    "\n",
    "# Summary\n",
    "In this tutorial, you discovered how to use gradient boosting models for classification and regression in Python.\n",
    "\n",
    "Specifically, you learned:\n",
    "\n",
    "* Gradient boosting is an ensemble algorithm that fits boosted decision trees by minimizing an error gradient.\n",
    "* How to evaluate and use gradient boosting with scikit-learn, including gradient boosting machines and the histogram-based algorithm.\n",
    "* How to evaluate and use third-party gradient boosting algorithms including XGBoost, LightGBM and CatBoost."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
